# goaurrpc
#### An implementation of the [aurweb](https://gitlab.archlinux.org/archlinux/aurweb) (v6) - /rpc - REST API service in go

This project implements the /rpc endpoints (REST API) as described [here](https://aur.archlinux.org/rpc/), as well as the "suggest" type.  
For the moment it should be considered being a proof-of-concept.  
While it should be fully functional, it still needs a bit more love (writing test, etc.) :wink:  

Main goal is to increase the performance of REST API.  

### Areas of improvement

In the current version (aurweb v6.0.13 was used for comparison/benchmarking), the bottleneck seems to be the database access.  
When a client makes a request an SQL statement is generated and a query is being run against the mariadb server.  
A pretty normal scenario in web application.

### Thoughts

##### In-Memory DB

Since the /rpc endpoint only returns data and does not need to perform any CUD operations, we could hold the package data in memory as well.

Like in the following scenario:

* Fetch all the necessary data from the database in keep it in memory.
* Assemble the data that is being returned to the client from the in-memory data.
* Periodically refresh the data

Doing that you don't need to bother about expensive database queries to your RDBMS.  
You'd run one big query every now and then to get fresh data...

Now for this POC we'll be using package data that is being composed by aurweb on a regular basis.  
Basically it's dumping package data into a gzip compressed JSON file.  
Turns out that this file does contain all the necessary information that is being exposed by the /rpc API. Nice!

### Approach

We periodically fetch the file (`packages-meta-ext-v1.json.gz`) with the package data and load it into memory.  
Instead of fetching data from the DB for each we request we utilize the in-memory data to compose our result and return it to the client.  
(In a "real" scenario you'd load this data directly from the DB; Does not matter for our performance tests though)

### Setup

For the sake of benchmarking I've installed aurweb on a bare metal system according to [these instructions](https://gitlab.archlinux.org/archlinux/aurweb/-/blob/master/TESTING)  
The machine is equipped with a 4 core Intel Core i5-4590T CPU, 16 GB RAM and and SSD. Pretty low-spec nowadays.  
Redis is used for caching (does not really make any difference. It seems in terms of the API endpoint it's only used to cache the rate-limit data)

The package data was generated by the `gendummydata.py` script, configured with 80K packages and 95K users.  
That pretty much matches the current numbers in the AUR.  
A small modification has been made to also generate descriptions for the packages otherwise they'd be empty in the DB (which has some effect on certain /rpc requests)  
This data was exported and used for the comparison, so that both, the FastAPI and goaurrpc have the same basis and can be compared properly.

The benchmarks are being run from a Zen2 8-core notebook. The connection between the machines is 1 GBit/s Ethernet.

### Benchmarks

Benchmarks were performed with the Apache Benchmark tool with a total of 1000 requests, running 10 threads in parallel.

To be published again.  
Previous benchmarks were done with just 2 FastAPI worker processes.  
New benchmark comparison with 5 workers (should be sufficient in a 4-core scenario) will be re-done soon.  

### Concerns

- No live data, since the data is being cached in memory and only reloaded every 5 minutes:  

Data could be reloaded more frequently.  
Loading the data from a JSON file or directly from the AUR webserver takes around 2 seconds.
Re-loading data every minute or even in a 10 second interval would be perfectly possible.  
That would only make sense if data is being retrieved directly from the DB though, the JSON file is only exported every 5 minutes...  

- Memory consumption?  

After startup, once all data is loaded the amount of memory that is allocated for the process is ~230 MB  
When data is being re-loaded periodically, the consumption increases temporarily to about ~430 MB   
until the "old set of data" is being garbage-collected.
Sometimes this might take a while. During my tests I have never seen to get bigger than ~500 MB  
(we could forcefully run the GC, but that does not really make sense.)

### How to build

- Download repository `git clone https://github.com/moson-mo/goaurrpc.git`
- `cd goaurrpc`
- Build with: `go build cmd/goaurrpc/goaurrpc.go`
- This will create a binary `goaurrpc`

### Config file

See `sample.conf` file. The config file can be loaded by specifying "-c" parameter when running goaurrpc.  
For example: `./goaurrpc -c sample.conf`.
If this parameter is not passed, the default config will be used (sample.conf contains the defaults).  

### Notes

Again this is just a quick and dirty POC implementation written in a couple of days.  
Needs some more work to get it "production-grade" ready :wink:

### Test endpoint

A goaurrpc endpoint can be found here for testing:
`http://server.moson.rocks:10666/rpc`